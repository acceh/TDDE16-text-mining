{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DzzdlXtdzIZb"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.layers import Dropout,LSTM\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy \n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_MxxuW2kCODY"
   },
   "source": [
    "# Preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k_btwIwl-MGH"
   },
   "outputs": [],
   "source": [
    "def preprocess(text):\r\n",
    "    return \" \".join([token.lemma_ for token in nlp(text) if not token.is_stop and token.is_ascii and not token.is_punct])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b-fU35Rx7hsr"
   },
   "outputs": [],
   "source": [
    "def preprocess_data(df):\r\n",
    "    tokenized_text = []\r\n",
    "    labels = []\r\n",
    "    for row in df.itertuples():\r\n",
    "          tokenized_text.append(preprocess(row[1]))\r\n",
    "          labels.append(row[2])\r\n",
    "\r\n",
    "  \r\n",
    "    features = {\"seq\": tokenized_text}\r\n",
    "    labels = {\"label\": labels}\r\n",
    "    return pandas.DataFrame(features),pandas.DataFrame(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "41pvkghyzgTJ"
   },
   "outputs": [],
   "source": [
    "df = pandas.read_csv(\"tripadvisor_hotel_reviews.csv\",encoding=\"utf8\")\n",
    " \n",
    "sequences, labels = preprocess_data(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "-Zpz-4LgIeed",
    "outputId": "9169da7a-29d1-4335-a1bd-e7388448ff6a"
   },
   "outputs": [],
   "source": [
    "labels[\"label\"].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VLTlekrR0lT5",
    "outputId": "c0602203-8890-42ea-9154-37b64f565d5b"
   },
   "outputs": [],
   "source": [
    "MAX_NB_WORDS = 5000\r\n",
    "MAX_SEQUENCE_LENGTH = 100\r\n",
    "EMBEDDING_DIM = 200\r\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\r\n",
    "tokenizer.fit_on_texts(sequences[\"seq\"])\r\n",
    "word_index = tokenizer.word_index\r\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5sj3ls0B0x3i",
    "outputId": "89e184d0-6226-41a6-f58f-3b54e14f7145"
   },
   "outputs": [],
   "source": [
    "X = tokenizer.texts_to_sequences(sequences[\"seq\"])\r\n",
    "X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\r\n",
    "print('Shape of data tensor:', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9vJFNWnW1FFv",
    "outputId": "773262df-5b09-4113-8485-c22c1283486b"
   },
   "outputs": [],
   "source": [
    "Y = pandas.get_dummies(labels[\"label\"]).values\r\n",
    "print('Shape of label tensor:', Y.shape)\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5Lu4CAz-1I8y",
    "outputId": "4dfedf60-9163-47f5-8e72-0d6af92e08c3"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.20)\r\n",
    "print(X_train.shape,Y_train.shape)\r\n",
    "print(X_test.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6pGeILNz2wLe",
    "outputId": "43cb28fa-a2d6-4976-aa07-f62ee96f51d6"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\r\n",
    "\r\n",
    "model.add(tf.keras.layers.Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\r\n",
    "model.add(Dropout(0.2))\r\n",
    "model.add(tf.keras.layers.LSTM(128))\r\n",
    "model.add(Dropout(0.5))\r\n",
    "model.add(tf.keras.layers.Dense(128, activation='relu'))\r\n",
    "model.add(tf.keras.layers.Dense(Y.shape[1], activation=\"softmax\"))\r\n",
    "\r\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n",
    "\r\n",
    "history = model.fit(X_train, Y_train, epochs=4, batch_size=64,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 754
    },
    "id": "U_WpfGB6LSlB",
    "outputId": "dbc8b315-a213-428d-a5e8-e821d1f2ccfe"
   },
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(\n",
    "    model,\n",
    "    to_file=\"model.png\",\n",
    "    show_shapes=True,\n",
    "    show_dtype=False,\n",
    "    show_layer_names=True,\n",
    "    rankdir=\"TB\",\n",
    "    expand_nested=True,\n",
    "    dpi=96,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tnyUZzQdgGD1",
    "outputId": "8c81325a-6e91-47bd-8317-5616971f194b"
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test, batch_size=64, verbose=1)\n",
    "y_pred_bool = np.argmax(y_pred, axis=1)\n",
    "\n",
    "decoded_Y_test = []\n",
    "\n",
    "for i in range(len(Y_test)):\n",
    "  decoded_Y_test.append(np.argmax(Y_test[i], axis=0))\n",
    "\n",
    "print(classification_report(decoded_Y_test, y_pred_bool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2IqYdcZR41BM",
    "outputId": "8625ed7c-03db-4da3-aa8a-a8a9391d9227"
   },
   "outputs": [],
   "source": [
    "eval = model.evaluate(X_test,Y_test)\r\n",
    "print(eval)\r\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}\\n'.format(eval[0],eval[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "id": "9zZjzM3W44YL",
    "outputId": "ff9037c6-f442-4918-e9c8-cd2c2a3f0b62"
   },
   "outputs": [],
   "source": [
    "plt.title('Loss')\r\n",
    "plt.plot(history.history['loss'], label='train')\r\n",
    "plt.plot(history.history['val_loss'], label='test')\r\n",
    "plt.legend()\r\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "id": "_jEki38z5G85",
    "outputId": "3696d8de-1ce5-4f99-8a02-d874a237195a"
   },
   "outputs": [],
   "source": [
    "plt.title('Accuracy')\r\n",
    "plt.plot(history.history['accuracy'], label='train')\r\n",
    "plt.plot(history.history['val_accuracy'], label='test')\r\n",
    "plt.legend()\r\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lD37XZ7ZewUw"
   },
   "source": [
    "# Creating baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UF1FiZ4meziN"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.dummy import DummyClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vMVZ3qh5e1_C",
    "outputId": "48dcb066-3b3a-46ad-ee42-840f57aae76b"
   },
   "outputs": [],
   "source": [
    "\n",
    "dummy_clf = DummyClassifier(strategy=\"uniform\")\n",
    "dummy_clf.fit(sequences, labels)\n",
    "\n",
    "dummy_clf.predict(sequences)\n",
    "\n",
    "dummy_clf.score(sequences, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F5Qk-wvG2ygX"
   },
   "source": [
    "# Naiver Bayes baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CCdYK1YI11Hf"
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "X_train_baseline, X_test_baseline, Y_train_baseline, Y_test_baseline = train_test_split(sequences[\"seq\"],labels[\"label\"], test_size = 0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "-iwiKiu7MDTa",
    "outputId": "376e19ba-8ac3-46dd-85d3-abfe23d2803e"
   },
   "outputs": [],
   "source": [
    "Y_train_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nmnozrke10_Z",
    "outputId": "406a85bf-92ae-44b2-95bb-ea8ae35ebe0d"
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline([('vectorizer', CountVectorizer()), ('naive', MultinomialNB())])\n",
    "tmp = pipe.fit(X_train_baseline, Y_train_baseline)\n",
    "pred = pipe.predict(X_test_baseline)\n",
    "score = pipe.score(X_test_baseline, Y_test_baseline)\n",
    "print(score)\n",
    "print(classification_report(Y_test_baseline, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BOyJWLX4SxX-"
   },
   "source": [
    "# Evaluation of hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ppq9n4z9UJU7",
    "outputId": "5ec4665c-eab1-4f41-8088-69b92fc986b6"
   },
   "outputs": [],
   "source": [
    "!pip install hyperas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qi8hhZg6T6F_"
   },
   "outputs": [],
   "source": [
    "\r\n",
    "# Sklearn tools\r\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\r\n",
    "from sklearn.model_selection import cross_val_score\r\n",
    "from sklearn.model_selection import KFold\r\n",
    "from sklearn.preprocessing import StandardScaler, LabelBinarizer\r\n",
    "from sklearn.pipeline import Pipeline\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "\r\n",
    "# Keras preprocessing, models, evaluators\r\n",
    "from keras.models import Sequential\r\n",
    "from keras.layers import Dense, Embedding, LSTM, GRU, Activation, Dropout, GlobalMaxPool1D, Conv1D\r\n",
    "from keras.layers.embeddings import Embedding\r\n",
    "from keras.optimizers import SGD\r\n",
    "from keras.utils.np_utils import to_categorical\r\n",
    "\r\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\r\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\r\n",
    "\r\n",
    "from keras.preprocessing import text, sequence\r\n",
    "from keras import utils\r\n",
    "from keras.models import load_model\r\n",
    "\r\n",
    "\r\n",
    "import hyperas\r\n",
    "from hyperas import optim\r\n",
    "from hyperas.distributions import choice, uniform\r\n",
    "from hyperopt import Trials, STATUS_OK, tpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WX8tMf9JS3ex"
   },
   "outputs": [],
   "source": [
    "\r\n",
    "def data():\r\n",
    "\r\n",
    "  import spacy\r\n",
    "  nlp = spacy.load(\"en_core_web_sm\")\r\n",
    "  df = pandas.read_csv(\"drive/MyDrive/Skola/TDDE16/tripadvisor_hotel_reviews.csv\",encoding=\"utf8\")\r\n",
    "  #sequences, labels = preprocess_data(reviews_df)\r\n",
    "\r\n",
    "  tokenized_text = []\r\n",
    "  labels = []\r\n",
    "  for row in df.itertuples():\r\n",
    "    tokenized_text.append(\" \".join([token.lemma_ for token in nlp(row[1]) if not token.is_stop and token.is_ascii and not token.is_punct]))\r\n",
    "    labels.append(row[2])\r\n",
    "  sequences = pandas.DataFrame({\"seq\": tokenized_text})\r\n",
    "  labels = pandas.DataFrame({\"label\": labels})\r\n",
    "\r\n",
    "\r\n",
    "  # The maximum number of words to be used. (most frequent)\r\n",
    "  MAX_NB_WORDS = 5000\r\n",
    "  # Max number of words in each complaint.\r\n",
    "  MAX_SEQUENCE_LENGTH = 100\r\n",
    "  # This is fixed.\r\n",
    "  EMBEDDING_DIM = 200\r\n",
    "  tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\r\n",
    "  tokenizer.fit_on_texts(sequences[\"seq\"])\r\n",
    "  word_index = tokenizer.word_index\r\n",
    "  print('Found %s unique tokens.' % len(word_index))\r\n",
    "  X = tokenizer.texts_to_sequences(sequences[\"seq\"])\r\n",
    "  X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\r\n",
    "  print('Shape of data tensor:', X.shape)\r\n",
    "  Y = pandas.get_dummies(labels[\"label\"]).values\r\n",
    "  print('Shape of label tensor:', Y.shape)\r\n",
    "\r\n",
    "\r\n",
    "  X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.10, random_state = 42)\r\n",
    "  print(X_train.shape,Y_train.shape)\r\n",
    "  print(X_test.shape,Y_test.shape)\r\n",
    "  return X_train, Y_train, X_test, Y_test\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8f7SiQB4Rec8"
   },
   "outputs": [],
   "source": [
    "def create_model(X_train, Y_train, X_test, Y_test):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Embedding(MAX_NB_WORDS, output_dim = 200 , input_length=X.shape[1]))\n",
    "    model.add(Dropout({{choice([0,0.2, 0.4, 0.5, 0.6])}}))\n",
    "    model.add(tf.keras.layers.LSTM(units = 100))\n",
    "    model.add(Dropout({{choice([0,0.2, 0.4, 0.5, 0.6])}}))\n",
    "    model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(Y.shape[1], activation=\"softmax\"))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    epochs = 4\n",
    "    batch_size = 128\n",
    "    result = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1)\n",
    "    validation_acc = np.amax(result.history['val_accuracy'])\n",
    "    print('Best validation acc of epoch:', validation_acc)\n",
    "    return {'loss': -validation_acc, 'status': STATUS_OK, 'model': model}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "2WYi1Kt6Ty1u",
    "outputId": "f88357b3-48cf-4a72-a35c-92fefd712a8c"
   },
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "  best_run, best_model, space = optim.minimize(model=create_model,\n",
    "                                           data=data,\n",
    "                                           algo=tpe.suggest,\n",
    "                                           max_evals=20,\n",
    "                                           trials=Trials(),\n",
    "                                           notebook_name='drive/MyDrive/Colab Notebooks/TDDE16_project',\n",
    "                                           eval_space=True,\n",
    "                                           return_space=True)\n",
    "except Exception as e:\n",
    "  print(e)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DwIKJJFrORes",
    "outputId": "e50f4a42-3f1a-42fd-c392-89f2c6ad288c"
   },
   "outputs": [],
   "source": [
    "X_train, Y_train, X_test, Y_test = data()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232
    },
    "id": "MDD7BYWO08pa",
    "outputId": "ed1ea509-f9a9-4d85-99b4-03a7efefe089"
   },
   "outputs": [],
   "source": [
    "print(\"Evalutation of best performing model:\")\r\n",
    "print(best_model.evaluate(X_test, Y_test))\r\n",
    "print(\"Best performing model chosen hyper-parameters:\")\r\n",
    "print(best_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h8foAbCzZCfw"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "id": "6iyoBqTxpcz2",
    "outputId": "d025e640-949f-4615-d8b1-4ed7260ce0c1"
   },
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(features_test, labels_test)\r\n",
    "\r\n",
    "print('Test Loss: {}'.format(test_loss))\r\n",
    "print('Test Accuracy: {}'.format(test_acc))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "TDDE16_project.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}